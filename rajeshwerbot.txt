Of course, Duncan. Here is the updated documentation with the application renamed to Rajeshwerbot.

Project Document: Rajeshwerbot
This document outlines the requirements and architecture for a local application designed to provide natural language interrogation, visualisation, and insight generation from business data.

1. Requirements Document
1.1. Introduction
The purpose of Rajeshwerbot is to provide business analysts and stakeholders with a powerful yet intuitive tool to analyse internal datasets. The application will leverage a local Large Language Model (LLM) to allow users to ask questions in natural language, view dynamic visualisations, and uncover trends, disruptions, and critical uncertainties within the data. The primary goal is to democratise data analysis while ensuring complete data privacy and security by processing everything on the user's local machine.

1.2. User Roles
Analyst: The primary user, who will interact with the application to explore data, generate insights, and prepare reports.

Stakeholder: A colleague who receives the packaged application to review specific datasets or findings.

1.3. Functional Requirements
FR1: Data Ingestion

The application must load data from local CSV files.

The system must be designed to accommodate future data sources, such as survey results.

The user shall be prompted to select a file for analysis upon starting the application.

FR2: Natural Language Querying

The application must feature a text input interface for users to ask questions about the data in plain English.

The system shall interpret queries related to trends, comparisons, filtering, and summarisation (e.g., "Which roles have the highest potential for automation in 2026?").

FR3: Data Visualisation

The system must dynamically generate appropriate charts (bar charts, line graphs, tables) in response to user queries.

Visualisations must be interactive (e.g., hovering to see data points).

FR4: Insight Generation

The application will use the LLM to provide narrative summaries and identify key insights based on the query results.

The system should be able to answer questions about macrotrends and disruptions as reflected in the data.

FR5: Conversation History

The application shall maintain a history of the current query-and-response session, allowing the user to ask follow-up questions.

FR6: Exporting

The user must be able to export visualisations as image files (e.g., PNG) and data tables as CSV files.

1.4. Non-Functional Requirements
NFR1: Performance: The application must provide a response (data + visualisation + insight) to a typical query in under 15 seconds on a standard corporate laptop.

NFR2: Usability: The user interface must be clean, intuitive, and require no prior technical training to use.

NFR3: Security & Privacy: All data, including the source files and user queries, must be processed and stored locally on the user's machine. No data shall be transmitted over any network.

NFR4: Deployability: The application must be packaged into a single executable file that can be easily shared and run by colleagues on Windows without requiring a separate installation of Python or other dependencies.

2. Architectural Document
2.1. System Overview
Rajeshwerbot will be a desktop application built using a 3-tier architecture. This structure separates the user interface, business logic, and data storage into distinct, manageable components. All tiers will be contained within the single deployable application, ensuring everything runs locally.

2.2. Component Breakdown
Tier 1: Presentation Layer (Frontend)

Framework: Streamlit.

Description: This layer is the user's sole point of interaction. Streamlit's widget-based system will be used to create the UI.

Key Components:

File Uploader: To select the CSV data source.

Chat Input: For users to type their natural language queries.

Conversation Window: To display the back-and-forth between the user and the application.

Visualisation Pane: An area where charts and tables generated by Plotly are rendered.

Insight Panel: A text area to display the narrative summary from the LLM.

Tier 2: Logic Layer (Backend)

Language/Platform: Python.

Description: This is the engine of the application. It processes user input, interacts with the LLM, performs data analysis, and prepares results for the frontend.

Key Components:

Streamlit Web Server: The built-in server that hosts the application logic and frontend.

Query Processor: Receives the raw text from the UI. It passes the query to the LLM for interpretation.

LLM Integration:

Model: Meta Llama 3.1 (8B) in GGUF format for efficiency.

Interface: A library like llama-cpp-python will be used to load the model into memory and run inference locally. This component will translate the user's natural language query into a structured data query and also generate the narrative summary.

Data Analysis Module:

Library: Pandas.

Function: Executes the structured query (e.g., filtering, grouping) on the data held in a Pandas DataFrame.

Visualisation Generator:

Library: Plotly.

Function: Creates interactive charts and graphs based on the results from the Data Analysis Module.

Tier 3: Data Layer

Technology: SQLite and Pandas DataFrames.

Description: This layer manages the data for the application.

Components:

Initial Data: Upon launch, the selected CSV is loaded into a Pandas DataFrame for high-speed analysis.

Database (Optional Extension): For more complex future requirements (e.g., managing multiple large datasets or surveys), the architecture supports integrating a local SQLite database to store and query the data.

Data Schema: The data will be structured based on the columns in the source CSV (e.g., Job_Role, Task, Automation_Potential_2023, etc.).

2.3. Data Flow
Initialisation: The user launches the application and uses the file uploader to load the ReclasNWNWx2_agentified - Sheet1.csv file into a Pandas DataFrame.

Query: The user types, "Show me the top 5 tasks with the highest automation potential in 2033" into the chat input.

Processing: The query is sent to the LLM Integration component. The LLM interprets the request and translates it into a structured command: sort by 'Automation_Potential_2033' descending, take top 5. It also prepares to summarise the findings.

Analysis: The Data Analysis Module (Pandas) executes this command on the DataFrame.

Visualisation: The resulting data is passed to the Visualisation Generator (Plotly), which creates a bar chart.

Response: The bar chart is rendered in the Visualisation Pane, and the LLM generates a text summary like, "The tasks with the highest automation potential in 2033 are primarily related to data entry and routine processing..." which is displayed in the Insight Panel. The original query and response are added to the conversation history.

2.4. Deployment Strategy
Tool: PyInstaller.

Process: A build script will be created to use PyInstaller to package the entire Python application, including the Streamlit framework, all dependent libraries, the Llama 3.1 model file, and the Python interpreter itself, into a single executable file (Rajeshwerbot.exe).

Distribution: This single file can be shared with colleagues, who can run it without any prerequisites or installation, ensuring a seamless user experience.